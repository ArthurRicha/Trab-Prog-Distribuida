#include <iostream>
#include <mpi.h>
#include <cmath>

constexpr int p_count = 512;

int rank, size;
int cutoff;
double min_x, max_x, min_y, max_y, dx, dy;

double modulus(double x, double y) {
    return sqrt(x * x + y * y);
}

void self_mul(double &x, double &y) {
    double ox = x * x - y * y;
    double oy = x * y + y * x;
    x = ox;
    y = oy;
}

void compute_mandelbrot_parallel(double *points, int npts, int *mset, int cutoff) {
    int points_per_process = npts / size;
    int start_index = rank * points_per_process;
    int end_index = (rank + 1) * points_per_process;

    // For each point assigned to this process
    for (int i = start_index; i < end_index; ++i) {
        double px, py;
        px = points[i * 2];
        py = points[i * 2 + 1];

        int iteration = 0;
        double zx = 0;
        double zy = 0;

        // We iterate until cutoff or modulus > 2
        while (iteration < cutoff) {
            self_mul(zx, zy);
            zx += px;
            zy += py;
            double mod = modulus(zx, zy);

            if (mod > 2.0)
                break;

            iteration++;
        }

        // We store the number of iterations, and we use
        // a special value (-1) if we don't converge
        if (iteration == cutoff)
            mset[i - start_index] = -1;
        else
            mset[i - start_index] = iteration;
    }
}

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (argc != 6) {
        if (rank == 0) {
            std::cerr << "Usage: " << argv[0] << " <min_x> <max_x> <min_y> <max_y> <cutoff>" << std::endl;
        }
        MPI_Finalize();
        return 1;
    }

    min_x = std::stod(argv[1]);
    max_x = std::stod(argv[2]);
    min_y = std::stod(argv[3]);
    max_y = std::stod(argv[4]);
    dx = max_x - min_x;
    dy = max_y - min_y;
    cutoff = std::stoi(argv[5]);

    double *points = nullptr;
    int *mset = nullptr;

    if (rank == 0) {
        // Initialization of the points only in process 0
        points = new double[p_count * p_count * 2];
        for (int yp = 0; yp < p_count; ++yp) {
            double py = min_y + dy * yp / p_count;
            for (int xp = 0; xp < p_count; ++xp) {
                double px = min_x + dx * xp / p_count;

                int idx = yp * p_count * 2 + xp * 2;
                points[idx] = px;
                points[idx + 1] = py;
            }
        }

        mset = new int[p_count * p_count];
    }

    // Broadcasting data to all processes
    MPI_Bcast(&cutoff, 1, MPI_INT, 0, MPI_COMM_WORLD);

    int points_per_process = p_count * p_count / size;
    double *local_points = new double[points_per_process * 2];

    // Scatter points to all processes
    MPI_Scatter(points, points_per_process * 2, MPI_DOUBLE, local_points, points_per_process * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    int *local_mset = new int[points_per_process];

    // Compute Mandelbrot set in parallel
    compute_mandelbrot_parallel(local_points, points_per_process * 2, local_mset, cutoff);

    int *gathered_mset = nullptr;
    if (rank == 0) {
        gathered_mset = new int[p_count * p_count];
    }

    // Gather results to process 0
    MPI_Gather(local_mset, points_per_process, MPI_INT, gathered_mset, points_per_process, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        // Printing the result
        for (int yp = 0; yp < p_count; ++yp) {
            for (int xp = 0; xp < p_count; ++xp)
                std::cout << gathered_mset[yp * p_count + xp] << " ";
            std::cout << std::endl;
        }

        delete[] gathered_mset;
    }

    delete[] points;
    delete[] mset;

    MPI_Finalize();
    return 0;
}
